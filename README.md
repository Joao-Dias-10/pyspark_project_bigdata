# ğŸš€ PySpark Big Data Pipeline

Projeto estruturado em PySpark, com foco em boas prÃ¡ticas, orientaÃ§Ã£o a objetos e escalabilidade para tratamento de grandes volumes de dados. A pipeline inclui carregamento, transformaÃ§Ã£o, limpeza e inserÃ§Ãµes otimizadas em bancos relacionais como PostgreSQL.

---

## ğŸ“Œ Objetivos

* Processar grandes volumes de dados de forma distribuÃ­da com PySpark e arquivos `.parquet`
* Utilizar orientaÃ§Ã£o a objetos (POO) para organizaÃ§Ã£o e reuso de cÃ³digo
* Aplicar boas prÃ¡ticas de engenharia de dados
* Realizar inserÃ§Ãµes eficientes em bancos relacionais (ex: PostgreSQL)
* Demonstrar o potencial do Apache Spark em ambientes de Big Data

---

## âš™ï¸ Tecnologias utilizadas

* Python 3.10+
* PySpark
* PostgreSQL
* Pytest
* SQLAlchemy (para modelagem ORM)
* notebooks (para testes e verificaÃ§Ãµes rÃ¡pidas)
* Dotenv (para gerenciamento de variÃ¡veis)
* Logging (para rastreamento de execuÃ§Ã£o)

---

## ğŸ§± Estrutura do projeto

```
â”œâ”€â”€ config/           # Arquivos de configuraÃ§Ã£o (.yaml, .env, etc.)
â”œâ”€â”€ data/             # Dados brutos e processados
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ logs/             # Logs de execuÃ§Ã£o
â”œâ”€â”€ notebooks/        # Cadernos Jupyter para testes manuais e EDA
â”œâ”€â”€ src/              # CÃ³digo-fonte principal
â”‚   â”œâ”€â”€ automation/   # Scripts de automaÃ§Ã£o 
â”‚   â”œâ”€â”€ db/           # LÃ³gica de banco de dados
â”‚   â”‚   â”œâ”€â”€ init_db.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ queries.py
â”‚   â”œâ”€â”€ preprocessing/ # PrÃ©-processamento e transformaÃ§Ã£o de dados
â”‚   â””â”€â”€ utils/        # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ tests/            # Testes unitÃ¡rios e de integraÃ§Ã£o
â”œâ”€â”€ .gitignore        # PadrÃµes de arquivos ignorados pelo Git
â”œâ”€â”€ main.py           # Ponto de entrada do projeto
â”œâ”€â”€ requirements.txt
```

---

### â–¶ï¸ Rodando o pipeline

```bash
python main.py
```

---

## ğŸ§ª Funcionalidades principais

* SQLAlchemy (para modelagem ORM)

* Download automÃ¡tico de arquivos .parquet via URL

* Leitura, limpeza e tratativa de dados com PySpark

* ConversÃµes de tipo, tratamento de nulos e colunas derivadas

* Salvamento em formato Parquet (com `coalesce` para gerar arquivo Ãºnico)

  > *Parquet Ã© um formato colunar, altamente eficiente para leitura e compressÃ£o, ideal para grandes volumes de dados e processamento distribuÃ­do.*

* InserÃ§Ã£o estruturada em PostgreSQL utilizando PySpark para escrita em lote

  âœ… **Desempenho**:
  Em testes reais, um arquivo `.parquet` contendo **3.970.553 linhas** (trÃªs milhÃµes, novecentos e setenta mil, quinhentos e cinquenta e trÃªs registros) foi processado em 30 segundos e **inserido no banco PostgreSQL em menos de 2 segundos**, comprovando a eficiÃªncia do pipeline mesmo com grandes volumes de dados.

---
