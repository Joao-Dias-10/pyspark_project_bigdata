# üöÄ PySpark Big Data Pipeline

Projeto estruturado em PySpark, com foco em boas pr√°ticas, orienta√ß√£o a objetos e escalabilidade para tratamento de grandes volumes de dados. A pipeline inclui carregamento, transforma√ß√£o, limpeza e inser√ß√µes otimizadas em bancos relacionais como PostgreSQL.

---

## üìå Objet ivos

- Processar grandes volumes de dados de forma distribu√≠da com PySpark e aquivos .parquet
- Utilizar orienta√ß√£o a objetos (POO) para organiza√ß√£o e reuso de c√≥digo
- Aplicar boas pr√°ticas de engenharia de dados
- Realizar inser√ß√µes eficientes em bancos relacionais (ex: PostgreSQL)
- Demonstrar o potencial do Apache Spark em ambientes de Big Data

---

## ‚öôÔ∏è Tecnologias utilizadas

- Python 3.10+
- PySpark
- PostgreSQL
- Pytest
- SQLAlchemy (para modelagem ORM)
- notebooks (para testes e verifica√ß√µes r√°pidas)
- Dotenv (para gerenciamento de vari√°veis)
- Logging (para rastreamento de execu√ß√£o)


---

## üß± Estrutura do projeto

```


‚îú‚îÄ‚îÄ config/           # Arquivos de configura√ß√£o (.yaml, .env, etc.)
‚îú‚îÄ‚îÄ data/             # Dados brutos e processados
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îú‚îÄ‚îÄ logs/             # Logs de execu√ß√£o
‚îú‚îÄ‚îÄ notebooks/        # Cadernos Jupyter para testes manuais e EDA
‚îú‚îÄ‚îÄ src/              # C√≥digo-fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ automation/   # Scripts de automa√ß√£o 
‚îÇ   ‚îú‚îÄ‚îÄ db/           # L√≥gica de banco de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ init\_db.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ queries.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/ # Pr√©-processamento e transforma√ß√£o de dados
‚îÇ   ‚îî‚îÄ‚îÄ utils/        # Fun√ß√µes utilit√°rias
‚îú‚îÄ‚îÄ tests/            # Testes unit√°rios e de integra√ß√£o
‚îú‚îÄ‚îÄ .gitignore        # Padr√µes de arquivos ignorados pelo Git
‚îú‚îÄ‚îÄ main.py           # Ponto de entrada do projeto
‚îú‚îÄ‚îÄ requirements.txt   
````

---

### ‚ñ∂Ô∏è Rodando o pipeline

```bash
python main.py
```

---

## üß™ Funcionalidades principais

* SQLAlchemy (para modelagem ORM)
* Download autom√°tico de arquivos .parquet via URL
* Leitura, limpeza e tratativa de dados com PySpark
* Convers√µes de tipo, tratamento de nulos e colunas derivadas
* Salvamento em formato Parquet (com coalesce para gerar arquivo √∫nico)

  > *Parquet √© um formato colunar, altamente eficiente para leitura e compress√£o, ideal para grandes volumes de dados e processamento distribu√≠do.*
* Inser√ß√£o estruturada em PostgreSQL utilizando PySpark para escrita em lote

---
