# ğŸš€ PySpark Big Data Pipeline

Projeto estruturado em PySpark, com foco em boas prÃ¡ticas, orientaÃ§Ã£o a objetos e escalabilidade para tratamento de grandes volumes de dados. A pipeline inclui carregamento, transformaÃ§Ã£o, limpeza e inserÃ§Ãµes otimizadas em bancos relacionais como PostgreSQL.

---

## ğŸ“Œ Objet ivos

- Processar grandes volumes de dados de forma distribuÃ­da com PySpark e aquivos .parquet
- Utilizar orientaÃ§Ã£o a objetos (POO) para organizaÃ§Ã£o e reuso de cÃ³digo
- Aplicar boas prÃ¡ticas de engenharia de dados
- Realizar inserÃ§Ãµes eficientes em bancos relacionais (ex: PostgreSQL)
- Demonstrar o potencial do Apache Spark em ambientes de Big Data

---

## âš™ï¸ Tecnologias utilizadas

- Python 3.10+
- PySpark
- PostgreSQL
- Pytest
- notebooks (para testes e verificaÃ§Ãµes rÃ¡pidas)
- dotenv (para gerenciamento de variÃ¡veis)
- logging (para rastreamento de execuÃ§Ã£o)

---

## ğŸ§± Estrutura do projeto

```


â”œâ”€â”€ config/           # Arquivos de configuraÃ§Ã£o (.yaml, .env, etc.)
â”œâ”€â”€ data/             # Dados brutos e processados
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ logs/             # Logs de execuÃ§Ã£o
â”œâ”€â”€ notebooks/        # Cadernos Jupyter para testes manuais e EDA
â”œâ”€â”€ src/              # CÃ³digo-fonte principal
â”‚   â”œâ”€â”€ automation/   # Scripts de automaÃ§Ã£o e agendamento
â”‚   â”œâ”€â”€ db/           # LÃ³gica de banco de dados
â”‚   â”‚   â”œâ”€â”€ connection.py
â”‚   â”‚   â”œâ”€â”€ init\_db.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ queries.py
â”‚   â”œâ”€â”€ preprocessing/ # PrÃ©-processamento e transformaÃ§Ã£o de dados
â”‚   â””â”€â”€ utils/        # FunÃ§Ãµes utilitÃ¡rias
â”œâ”€â”€ tests/            # Testes unitÃ¡rios e de integraÃ§Ã£o
â”œâ”€â”€ .gitignore        # PadrÃµes de arquivos ignorados pelo Git
â”œâ”€â”€ main.py           # Ponto de entrada do projeto
â”œâ”€â”€ requirements.txt   
````

---

### â–¶ï¸ Rodando o pipeline

```bash
python main.py
```

---

## ğŸ§ª Funcionalidades principais

* Download automÃ¡tico de arquivos `.parquet` via URL
* Leitura, limpeza e tratativa de dados com PySpark
* ConversÃµes de tipo, tratamento de nulos e colunas derivadas
* Salvamento em formato Parquet (com coalesce para arquivo Ãºnico)
* InserÃ§Ã£o estruturada em PostgreSQL 

---

